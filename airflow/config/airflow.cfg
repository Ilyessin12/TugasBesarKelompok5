[core]
# The folder where your airflow pipelines live
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# The executor class that airflow should use
executor = CeleryExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow
load_examples = False

# Whether to load the default connections that ship with Airflow
load_default_connections = True

# The timezone used to render timestamps
default_timezone = utc

[webserver]
# The base url of your website
base_url = http://localhost:8080

# The port on which to run the web server
web_server_port = 8080

# Secret key used to run your flask app
secret_key = efe3a08596df5aca2170113315e484dc80d1ae5562ea94396c80765f4c09

# Number of workers to run the Gunicorn web server
workers = 4

# Access log format
access_logformat = %%(h)s %%(l)s %%(u)s %%(t)s "%%(r)s" %%(s)s %%(b)s %%(L)s %%(f)s

# Expose the configuration file in the web server
expose_config = False

# Set to true to turn on authentication
authenticate = True

# Number of seconds the webserver waits before timing out
web_server_worker_timeout = 120

[scheduler]
# Task instances listen for external kill signal (when you clear tasks from the CLI or the UI)
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the DAG for new tasks)
scheduler_heartbeat_sec = 5

# The number of task instances to process at once
max_threads = 2

# How many seconds to wait between file-parsing loops to prevent the logs from being spammed
min_file_process_interval = 30

# How often in seconds to scan the DAGs directory for new files
dag_dir_list_interval = 300

# How many seconds do we wait for tasks to heartbeat before they are marked as zombies
scheduler_zombie_task_threshold = 300

[celery]
# This section only applies if you're using the CeleryExecutor
broker_url = redis://:@redis:6379/0
result_backend = db+postgresql://airflow:airflow@postgres/airflow

[api]
# How to authenticate users of the API
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

[smtp]
# If you want airflow to send emails on retries, failure, and success
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_user = airflow
smtp_password = airflow
smtp_port = 25
smtp_mail_from = airflow@example.com

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Logging level
logging_level = INFO

# Log format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index >= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
